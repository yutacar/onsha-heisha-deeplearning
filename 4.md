##### オンシャヘイシャ勉強会
#ニューラルネットワークの学習

## キーワード
+ 損失関数
    + 二乗和誤差
    + 交差エントロピー誤差
+ ミニバッチ
+ 勾配
+ 勾配降下法

## 学習アルゴリズムの実装

### 前提
ニューラルネットワークには、適応可能な重みとバイアスがあり、この重みとバイアスを訓練データに適応するように調整することを「学習」と呼ぶ。

### ステップ１（ミニバッチ）
+ 訓練データからランダムに一部のデータを選び出す
+ ミニバッチの損失関数の値を減らすことを目的とする

### ステップ２（勾配の算出）
+ ミニバッチの損失関数を減らすために、各重みパラメータの勾配を求める
+ 勾配は、損失関数の値を最も減らす方向を示す

### ステップ３（パラメータの更新）
+ 重みパラメータを勾配方向に微小量だけ更新

### ステップ４（繰り返す）
+ ステップ１−３を繰り返す

確率的勾配降下法（stochastic gradient descent）と呼ばれ、無作為にに選び出したデータに対して行う勾配降下法。
頭文字を取ってSGDという名前が出実装される。

## まとめ
+ 機械学習で使用するデータセットは、訓練データとテストデータに分けて使用する
+ 訓練データで学習を行い、学習したモデルの汎化能力をテストデータを評価する
+ ニューラルネットワークの学習は、損失関数を指標として、損失関数の値が小さくなるように、重みパラメータを更新する
+ 重みパラメータを更新する際には、重みパラメータの勾配を利用して、勾配方向に重みの値を更新する作業を繰り返す
+ 微小な値を与えたときの差分によって微分を求めることを数値微分と言う
+ 数値微分によって、重みパラメータの勾配をも求めることができる
+ 数値微分による計算には時間がかかるが、その実装は簡単である。