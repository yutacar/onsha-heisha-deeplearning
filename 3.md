##### オンシャヘイシャ勉強会
#ニューラルネットワーク
## パーセプトロンの復讐
### 良い点
+ コンピュータが行うような複雑な処理も表現可能
### 悪い点
+ 重みを設定するのは人の手

→悪い点を解決するためにニューラルネットワークがある

→具体的にいうと、適切な重みパラメータをデータから自動で解決できる

##　パーセプトロンからニューラルネットワーク
+ 多くの共通点がある
### ニューラルネットワークの例
![ニューラルネットワークの例](3-1.png)
+ ニューロンのつながり方にはパーセプトロンと違いはない
+ キーワード
    + 入力層
    + 中間層（隠れ層）
    + 出力層
### パーセプトロンの復讐
![パーセプトロン](2-1.png)

![パーセプトロン式](2-2.png)

バイアスbを１として図示する。バイアスは固定値のため、差別化するため、灰色に塗った
![バイアスを明示的に示す](3-2.png)

h(x)という関数を導入し、式を変換する
![式を書き直す１](3-3.png)

![式を書き直す２](3-4.png)
入力信号の総和がh(x)という関数で変換された

### 活性化関数

### シグモイド関数
### ステップ関数の実装
### シグモイド関数の実装
### シグモイド関数とステップ関数の比較
### ReLU関数
### ニューラルネットワークの内積
## 分類問題と回帰問題
### 恒等関数とソフトマックス関数
## 手書き数字認識
### MNISTデータセット
### ニューラルネットワークの推論処理
### バッチ処理
### まとめ
+ パーセプトロンと、ニューロンの信号が階層的に伝わるという点で同じ
+ ニューロンへ信号を送信する際に、信号を変化させる活性化関数に大きな違いがある
+ ニューラルネットワークでは活性化関数が滑らかに変化するシグモイド関数、パーセプトロンでは信号が急に変化するステップ関数を使用
+ ニューラルネットワークでは、活性化関数としてシグモイド関数やReLU関数のような滑らかに変換する関数を利用
+ 機械学習の問題は、回帰問題と分類問題に大別できる
+ 出力層で使用する活性化関数は、回帰問題では恒等関数、分類問題ではソフトマックス関数を一般的に利用する
+ 分類問題では、出力層のニューロンの数を分類するクラス数に設定
+ 入力データのまとまりをバッチと言い、バッチ単位で推論処理を行うことで、計算を高速に行える